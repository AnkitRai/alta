{
 "metadata": {
  "name": "",
  "signature": "sha256:668981e01d3ea37e254be05457308c0874689aa2c93f4c26d884b1f8912c2b79"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting NLTK for Text Processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook introduced the Natural Language Toolkit (NLTK) which facilitates a braod range of tasks for text processing and representing results. It's part of the [The Art of Literary Text Analysis](ArtOfLiteraryTextAnalysis.ipynb) and assumes that you've already worked through previous notebooks ([Getting Setup](GettingSetup.ipynb), [Getting Started](GettingStarted.ipynb) and [Getting Texts](GettingTexts.ipynb)). In this notebook we'll look in particular at\n",
      "\n",
      "* installing the nltk library (for text processing)\n",
      "* simple tokenization of words\n",
      "* producing a simple table of frequencies of words\n",
      "* applying a list of stopwords (words to ignore)\n",
      "* producing a simple concordance for a keyword"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Installing the NLTK Library"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you installed Anaconda during the intial setup, then the NLTK library should already be present.\n",
      "\n",
      "If not, you'll need to open a terminal window (as we did in [Getting Setup](GettingSetup.ipynb)) and give the following command:\n",
      "\n",
      "> sudo pip3 install -U nltk\n",
      "\n",
      "Once the installation is complete, we can proceed within a iPython notebook. In a code cell of a temporary notebook (create a new one if needed), paste the following and then run the cell (shift-enter):\n",
      "\n",
      "> ```python\n",
      "import nltk\n",
      "nltk.download()```\n",
      "\n",
      "This should cause a new window to appear (eventually) with a dialog box to download data collections. For the sake of simplicity, if possible select the \"all\" row and press \"Download\". Once the download is complete, you can close that window.\n",
      "\n",
      "![NLTK Data Download](images/nltk-data-download.png)\n",
      "\n",
      "Now you're set! You can close and delete the temporary notebook used for installation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text Processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have NLTK installed, let's use it for text processing.\n",
      "\n",
      "We'll start by retrieving _The Gold Bug_ plain text that we had saved locally in the [Getting Texts](GettingTexts.ipynb) notebook. If you need to recapitulate the essentials of the previous notebook, try running this to retrieve the text:\n",
      "\n",
      "```python\n",
      "import urllib.request\n",
      "# retrieve Poe plain text value\n",
      "poeUrl = \"http://www.gutenberg.org/cache/epub/2147/pg2147.txt\"\n",
      "poeString = urllib.request.urlopen(poeUrl).read().decode()```\n",
      "\n",
      "And then this, in a separate cell so that we don't read repeatedly from Gutenberg:\n",
      "\n",
      "```python\n",
      "import os\n",
      "# isolate The Gold Bug\n",
      "start = poeString.find(\"THE GOLD-BUG\")\n",
      "end = poeString.find(\"FOUR BEASTS IN ONE\")\n",
      "goldBugString = poeString[start:end]\n",
      "# save the file locally\n",
      "directory = \"data\"\n",
      "if not os.path.exists(directory):\n",
      "    os.makedirs(directory)\n",
      "f = open(\"data/goldBug.txt\", \"w\")\n",
      "f.write(goldBugString)\n",
      "f.close()```\n",
      "\n",
      "Now we should be ready to retrieve the text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"data/goldBug.txt\", \"r\")\n",
      "goldBugString = f.read()\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we will work toward showing the top frequency words in our plain text. This involves three major steps:\n",
      "\n",
      "1. processing our plain text to find the words (also known as tokenization)\n",
      "1. counting the frequencies of each word\n",
      "1. displaying the frequencies information"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tokenization is the basic process of parsing a string to divide it into smaller units of the same kind. You can tokenize text into paragraphs, sentences, words or other structures, but here we're focused on recognizing words in our text. For that, let's import the ```nltk``` library and use its convenient ```word_tokenize()``` function. NLTK actually has several ways of tokenizing texts, and for that matter we could write our own code to do it. We'll have a peek at the first ten tokens."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "goldBugTokens = nltk.word_tokenize(goldBugString)\n",
      "goldBugTokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "['THE', 'GOLD-BUG', 'What', 'ho', '!', 'what', 'ho', '!', 'this', 'fellow']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see from the above that ```word_tokenize``` does a useful job of identifying words (including hyphenated words like \"GOLD-BUG\", but also includes tokens like the exclamation mark. In some cases punctuation like this might be useful, but in our case we want to focus on word frequencies, so we should filter out punctuation tokens.\n",
      "\n",
      "To accomplish the filtering we will use a construct called [list comprehension](https://docs.python.org/3.4/tutorial/datastructures.html#list-comprehensions) with a conditional test built in. Let's take it one step at a time, first using a loop structure like we've already seen in [Getting Texts](GettingTexts.ipynb), and then doing the same thing with a list comprehension."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loop\n",
      "loopList = []\n",
      "for word in goldBugTokens[:10]:\n",
      "    loopList.append(word)\n",
      "print(loopList)\n",
      "    \n",
      "    \n",
      "# list comprehension\n",
      "print([word for word in goldBugTokens[:10]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['THE', 'GOLD-BUG', 'What', 'ho', '!', 'what', 'ho', '!', 'this', 'fellow']\n",
        "['THE', 'GOLD-BUG', 'What', 'ho', '!', 'what', 'ho', '!', 'this', 'fellow']\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Identical! So the general form of a list comprehension (which is very compact) is: \n",
      "\n",
      "> [_expression(item)_ for _item_ in _list_)]\n",
      "\n",
      "We can now go a step further and add a condition to the list comprehension: we'll only include the word in the final list if any of the characters in the word is alphabetic as defined by the [isalpha()](https://docs.python.org/3.4/library/stdtypes.html?highlight=isalpha#str.isalpha) function. The ```any()``` function simply confirms that at least one of the characters in the word is a list. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print([word for word in goldBugTokens[:10] if any(c.isalpha() for c in word)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['THE', 'GOLD-BUG', 'What', 'ho', 'what', 'ho', 'this', 'fellow']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've now stripped out the exclamation mark (while keeping other word tokens that contain punctuation like a hyphen).\n",
      "\n",
      "Note that our syntax above is a bit tricky since we now have two list comprehensions, first for the words, and then for the characters in the ```any()``` condition. We could perform a similar test without a second list comprehension \u2013 using a regular expression to find word characters, for instance \u2013\u00a0but this has the benefit of showing how list comprehension can work on both lists (like word tokens) and strings (a sequence of characters)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Word Frequencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've had a first pass at word tokenization (keeping only word tokens), let's look at counting word frequencies. Essentially we want to go through the tokens and tally the number of times each one appears. Not surprisingly, the NLTK has a very convenient method for doing just this, which we can see in this small sample (the first 10 word tokens):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugRealWordTokensSample = [word for word in goldBugTokens[:10] if any(c.isalpha() for c in word)]\n",
      "goldBugRealWordFrequencies = nltk.FreqDist(goldBugRealWordTokensSample)\n",
      "goldBugRealWordFrequencies"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "FreqDist({'ho': 2, 'What': 1, 'this': 1, 'fellow': 1, 'GOLD-BUG': 1, 'what': 1, 'THE': 1})"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This ```FreqDist``` object is a kind of dictionary, where each word is paired with its frequency (separated by a colon), and each pair is separated by a comma. This kind of dictionary also has a very convenient way of displaying results as a table:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugRealWordFrequencies.tabulate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  ho What this fellow GOLD-BUG what  THE \n",
        "   2    1    1    1    1    1    1 \n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results are displayed in descending order of frequency (two occurrences of \"ho\"). One of the things we can notice is that \"What\" and \"what\" are calculated separately, which in some cases may be good, but for our purposes probably isn't. This might lead us to rethink our steps until now and consider the possibility of converting our string to lowercase during tokenization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugTokensLowercase = nltk.word_tokenize(goldBugString.lower()) # use lower() to convert entire string to lowercase\n",
      "goldBugRealWordTokensLowercaseSample = [word for word in goldBugTokensLowercase[:10] if any(c.isalpha() for c in word)]\n",
      "goldBugRealWordFrequenciesSample = nltk.FreqDist(goldBugRealWordTokensLowercaseSample)\n",
      "goldBugRealWordFrequencies.tabulate(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  ho What this fellow GOLD-BUG what  THE \n",
        "   2    1    1    1    1    1    1 \n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good, now we have \"what\" and \"What\" as the same word form counted twice. (There are disadvantages to this as well, such as more difficulty in identifying proper names and the start of sentences, but text mining is often a set of compromises.)\n",
      "\n",
      "Let's redo our entire workflow with the full set of tokens (not just a sample)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugTokensLowercase = nltk.word_tokenize(goldBugString.lower())\n",
      "goldBugRealWordTokensLowercase = [word for word in goldBugTokensLowercase if any(c.isalpha() for c in word)]\n",
      "goldBugRealWordFrequencies = nltk.FreqDist(goldBugRealWordTokensLowercase)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One simple way of measuring the vocabulary richness of an author is to calculate the ratio of the total number of words and the number of unique words. If an author repeats words more often, it may be because he or she is drawing on a smaller vocabulary (either deliberately or not), which is a measure of style. There are several factors to consider, such as the length of the text, but in the simplest terms a we can calculate the lexical diversity of an author by diving the number word forms (types) by the total number of tokens. We already have the necessary ingredients:\n",
      "\n",
      "* types: number of different words (number of word: count pairs in ```goldBugRealWordFrequencies```)\n",
      "* tokens: total number of word tokens (length of ```goldBugRealWordTokensLowercase```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"number of types: \", len(goldBugRealWordFrequencies))\n",
      "print(\"number of tokens: \", len(goldBugRealWordTokensLowercase))\n",
      "print(\"type/token ratio: \", len(goldBugRealWordFrequencies)/len(goldBugRealWordTokensLowercase))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of types:  2733\n",
        "number of tokens:  13639\n",
        "type/token ratio:  0.20038125962313952\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We haven't yet looked at our output for the top frequency lowercase words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugRealWordFrequencies.tabulate(20) # show a sample of the top frequency terms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " the   of  and    i   to    a   in   it  you  was that with  for   as  had   at   he this  but   we \n",
        " 877  465  359  336  329  327  238  213  162  137  130  114  113  113  110  108  103   99   99   98 \n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We tokenized, filtered and counted in three lines of code, and then a fourth to show the top frequency terms, but the results aren't necessarily very exciting. There's not much in these top frequency words that could be construed as especially characteristic of _The Gold Bug_, in large part because the most frequent words are similar for most texts of a given language: they're so-called function words that have more of a syntactic (grammatical) function rather than a semantic (meaning-bearing) value.\n",
      "\n",
      "Fortunately, our NLTK library contains a list of stopwords for English (and other languages). We can load the list and look at its contents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = nltk.corpus.stopwords.words(\"English\")\n",
      "print(sorted(stopwords))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 's', 'same', 'she', 'should', 'so', 'some', 'such', 't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can test whether one word is an item in another list with the following syntax."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"sample words: \", goldBugRealWordTokensLowercaseSample)\n",
      "print(\"sample words not in stopwords list: \", [word for word in goldBugRealWordTokensLowercaseSample if not word in stopwords])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sample words:  ['the', 'gold-bug', 'what', 'ho', 'what', 'ho', 'this', 'fellow']\n",
        "sample words not in stopwords list:  ['gold-bug', 'ho', 'ho', 'fellow']\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we can now tweak our word filter with an additional condition, adding the ```and``` operator between the test for alphabetic characters and the test for presence in the stopword list. We add a slash (\\) character to treat the code as if it were on one line. Alternatively, we could have done this in two steps (perhaps less efficient but arguably easier to read):\n",
      "\n",
      "```python\n",
      "# first filter tokens with alphabetic characters\n",
      "goldBugRealContentWordTokensLowercase = [word for word in goldBugTokensLowercase if any(c.isalpha() for c in word)]\n",
      "# then filter stopwords\n",
      "goldBugRealContentWordTokensLowercase = [word for word in goldBugRealContentWordTokensLowercase if word not in stopwords]```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugRealContentWordTokensLowercase = [word for word in goldBugTokensLowercase \\\n",
      "        if any(c.isalpha() for c in word) and word not in stopwords]\n",
      "goldBugRealContentWordFrequencies = nltk.FreqDist(goldBugRealContentWordTokensLowercase)\n",
      "goldBugRealContentWordFrequencies.tabulate(10) # show a sample of the top "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "upon   de   's jupiter legrand  one well said massa could \n",
        "  81   73   56   53   47   38   35   35   34   33 \n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have words that seem a bit more meaningful (even if the table format is a bit off). The first word (\"upon\") could be considered a function word (a preposition) that should be in the stopword list, though it's less common in modern English. The second word (\"de\") would be in French stopword list, but seems striking here in English. The third word \"'s\" is actually an artifact of possessive forms \u2013\u00a0sometimes tokenization keeps possessives together with the word, sometimes not. The next words (\"jupiter\" and \"legrand\") merit closer inspection, they may be proper names that have been transformed to lowercase. We can continue on like this with various observations and hypotheses, but really we probably want to have a closer look at individual occurences to see what's happening. For that, we'll build a concordance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building a Simple Concordance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A concordance allows us to see each occurrence of a term in its context. It has a rich history in textual scholarship, dating back to well before the advent of computers. It's a tool for studying word usage in context.\n",
      "\n",
      "The easiest way to build a concordance is to create an NLTK Text object from a list of word tokens (in this case we'll use the unfiltered list so that we can better read the text). So, for instance, we can ask for a concordance of \"de\" to try to better understand why it occurs so often in this English text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "goldBugText = nltk.Text(goldBugTokens)\n",
      "goldBugText.concordance(\"de\", lines=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 10 of 73 matches:\n",
        "ou , '' here interrupted Jupiter ; `` de bug is a goole bug , solid , ebery bi\n",
        "is your master ? '' `` Why , to speak de troof , massa , him not so berry well\n",
        " aint find nowhar -- dat 's just whar de shoe pinch -- my mind is got to be be\n",
        "taint worf while for to git mad about de matter -- Massa Will say noffin at al\n",
        " -- Massa Will say noffin at all aint de matter wid him -- but den what make h\n",
        "a gose ? And den he keep a syphon all de time -- '' '' Keeps a what , Jupiter \n",
        " , Jupiter ? '' `` Keeps a syphon wid de figgurs on de slate -- de queerest fi\n",
        "' `` Keeps a syphon wid de figgurs on de slate -- de queerest figgurs I ebber \n",
        " syphon wid de figgurs on de slate -- de queerest figgurs I ebber did see . Is\n",
        "vers . Todder day he gib me slip fore de sun up and was gone de whole ob de bl\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some tasks to try:\n",
      "\n",
      "* show a table of the top 20 words\n",
      "    * choose 3 words to add to the stopwords list using list concatenation\n",
      "    * regenerate the list of the top 20 words using your new stopwords list\n",
      "* instead of testing for presence in the stopword list, how would test for words that contain 10 characters or more?\n",
      "* determine whether or not the word provided to the concordance function is case sensitive\n",
      "\n",
      "In the next notebook we're going to introduce how to produce charts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "From [The Art of Literary Text Analysis](ArtOfLiteraryTextAnalysis.ipynb) by [St\u00e9fan Sinclair](http://stefansinclair.name) &amp; [Geoffrey Rockwell](http://geoffreyrockwell.com), [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)<br />\n",
      "Created January 19, 2015"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}